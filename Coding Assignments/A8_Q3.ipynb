{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mgvw5RQI66ed",
        "outputId": "2eae448c-9480-4f8a-c7e0-263ea3861d04"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# may need this\n",
        "# import os\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "metadata": {
        "id": "2G40MbuMHhja"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KH__R5Tp6xYG"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "dataset = torchvision.datasets.VOCDetection(\n",
        "    root=\"./PASCAL VOC/\", year='2012', image_set='trainval', download=False,\n",
        "    transform=torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize(size=(300,300)),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        # data is already in [0, 1], so normalize it to [-1, 1]\n",
        "        torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "    ])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THbL-3e97DmL",
        "outputId": "75c7665c-6f3f-4714-b702-64b8c1615672"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset VOCDetection\n",
              "    Number of datapoints: 11540\n",
              "    Root location: ./PASCAL VOC/\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Resize(size=(300, 300), interpolation=bilinear, max_size=None, antialias=True)\n",
              "               ToTensor()\n",
              "               Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.detection.faster_rcnn.fasterrcnn_resnet50_fpn_v2(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT)"
      ],
      "metadata": {
        "id": "QLIPlW7H7EKz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
        "    model.roi_heads.box_predictor.cls_score.in_features,\n",
        "    20+1  # +1 for background\n",
        ")"
      ],
      "metadata": {
        "id": "kByiPw-C7FbO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dataloader_dataset = torch.utils.data.DataLoader(dataset, batch_size=16, collate_fn=lambda x: x, shuffle=True)"
      ],
      "metadata": {
        "id": "5GMw6_Go7G6y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# need to convert label to integer for training\n",
        "classes = set([f.split('_')[0] for f in os.listdir(f\"{dataset.root}/VOCdevkit/VOC2012/ImageSets/Main/\") if '_' in f])\n",
        "classes = list(classes)\n",
        "class_to_index = {class_:i for i, class_ in enumerate(classes, start=1)}\n",
        "class_to_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg5D0mPa7H4r",
        "outputId": "b8f49df2-627a-445e-baf3-ac72c23905af"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bottle': 1,\n",
              " 'pottedplant': 2,\n",
              " 'chair': 3,\n",
              " 'bird': 4,\n",
              " 'bus': 5,\n",
              " 'cat': 6,\n",
              " 'dog': 7,\n",
              " 'car': 8,\n",
              " 'cow': 9,\n",
              " 'boat': 10,\n",
              " 'person': 11,\n",
              " 'sofa': 12,\n",
              " 'train': 13,\n",
              " 'sheep': 14,\n",
              " 'tvmonitor': 15,\n",
              " 'horse': 16,\n",
              " 'bicycle': 17,\n",
              " 'diningtable': 18,\n",
              " 'motorbike': 19,\n",
              " 'aeroplane': 20}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for finetuning, only train roi_head parameters\n",
        "for name, param in model.named_parameters():\n",
        "  if 'roi_head' not in name:\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "OqsyrGPr7Seu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYJgGYft879R",
        "outputId": "291f705e-a45a-4ec4-94a7-3f7befb1eae1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm data is in [-1, 1]\n",
        "for data in dataloader_dataset:\n",
        "  for d in data:\n",
        "    d = d[0]\n",
        "    print(d.min(), d.max())\n",
        "    break\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elQdpaelC8mv",
        "outputId": "881729c4-f2cd-41bb-d976-d133a8d19457"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-0.9922) tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "\n",
        "opt = torch.optim.SGD(params=model.parameters())\n",
        "bce = torch.nn.CrossEntropyLoss()\n",
        "mse = torch.nn.L1Loss()\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "n_epochs = 1\n",
        "total_batches = len(dataloader_dataset)\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    start = time()\n",
        "    print(f\"Epoch {epoch}/{n_epochs}\")\n",
        "\n",
        "    classifier_loss_per_epoch = 0\n",
        "    bbox_loss_per_epoch = 0\n",
        "\n",
        "    for i, data in enumerate(dataloader_dataset, start=1):\n",
        "        print(f\"\\t- Step {i}/{total_batches}\", end='')\n",
        "\n",
        "        X = []\n",
        "        y = []\n",
        "        for d in data:\n",
        "            objs = d[1]['annotation']['object']\n",
        "            bboxes = []\n",
        "            labels = []\n",
        "            for i in range(len(objs)):\n",
        "                bbox_dict = objs[i]['bndbox']\n",
        "                # must be in this order or height/width may be negative due to wrong order\n",
        "                bbox = [int(bbox_dict[key]) for key in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
        "                bboxes.append(bbox)\n",
        "\n",
        "                label = objs[i]['name']\n",
        "                label = class_to_index[label]\n",
        "                labels.append(label)\n",
        "\n",
        "            bboxes = torch.as_tensor(bboxes, dtype=torch.int64)\n",
        "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "            # need to move data to GPU using cuda()\n",
        "            X.append(d[0].cuda())\n",
        "            y.append({\n",
        "                'boxes': bboxes.cuda(),\n",
        "                'labels': labels.cuda(),\n",
        "            })\n",
        "\n",
        "        loss_dict = model(X, y)\n",
        "        loss = sum(v for v in loss_dict.values())\n",
        "        # print(loss_dict)\n",
        "        classifier_loss_per_epoch += loss_dict['loss_classifier']\n",
        "        bbox_loss_per_epoch += loss_dict['loss_box_reg']\n",
        "\n",
        "        print(f\" - Losses: {loss_dict['loss_classifier']} + {loss_dict['loss_box_reg']}\", end='\\n')\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "    end = time()\n",
        "    print(f\"Classifier Loss: {classifier_loss_per_epoch}\")\n",
        "    print(f\"BBox Regression Loss: {bbox_loss_per_epoch}\")\n",
        "    print(f\"Took {end - start}s\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nYRJvar7I__",
        "outputId": "081cbd6b-b995-45f0-d677-f11bec59f8ff"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1\n",
            "\t- Step 1/722 - Losses: 2.809926748275757 + 0.059281934052705765\n",
            "\t- Step 2/722 - Losses: 2.6568870544433594 + 0.025595858693122864\n",
            "\t- Step 3/722 - Losses: 2.5669920444488525 + 0.05643651261925697\n",
            "\t- Step 4/722 - Losses: 2.3812992572784424 + 0.06163829192519188\n",
            "\t- Step 5/722 - Losses: 2.28155779838562 + 0.03883455693721771\n",
            "\t- Step 6/722 - Losses: 2.1434621810913086 + 0.02869715541601181\n",
            "\t- Step 7/722 - Losses: 2.0219149589538574 + 0.03044021874666214\n",
            "\t- Step 8/722 - Losses: 1.9284614324569702 + 0.04599500447511673\n",
            "\t- Step 9/722 - Losses: 1.7847416400909424 + 0.035258494317531586\n",
            "\t- Step 10/722 - Losses: 1.6808841228485107 + 0.043107181787490845\n",
            "\t- Step 11/722 - Losses: 1.6064794063568115 + 0.04273711144924164\n",
            "\t- Step 12/722 - Losses: 1.4956637620925903 + 0.07307571917772293\n",
            "\t- Step 13/722 - Losses: 1.4263076782226562 + 0.06863649189472198\n",
            "\t- Step 14/722 - Losses: 1.3210011720657349 + 0.038249120116233826\n",
            "\t- Step 15/722 - Losses: 1.2404628992080688 + 0.061552368104457855\n",
            "\t- Step 16/722 - Losses: 1.196035385131836 + 0.10936252772808075\n",
            "\t- Step 17/722 - Losses: 1.0824846029281616 + 0.06056167185306549\n",
            "\t- Step 18/722 - Losses: 1.0173513889312744 + 0.05102074146270752\n",
            "\t- Step 19/722 - Losses: 0.9138029217720032 + 0.025359727442264557\n",
            "\t- Step 20/722 - Losses: 0.9323107600212097 + 0.0619887113571167\n",
            "\t- Step 21/722 - Losses: 0.8430249691009521 + 0.0572827011346817\n",
            "\t- Step 22/722 - Losses: 0.8194692730903625 + 0.06061454117298126\n",
            "\t- Step 23/722 - Losses: 0.7604103088378906 + 0.027382679283618927\n",
            "\t- Step 24/722 - Losses: 0.6936119794845581 + 0.029535530135035515\n",
            "\t- Step 25/722 - Losses: 0.6700775027275085 + 0.059123896062374115\n",
            "\t- Step 26/722 - Losses: 0.6087490916252136 + 0.028035683557391167\n",
            "\t- Step 27/722 - Losses: 0.6282153725624084 + 0.05223026126623154\n",
            "\t- Step 28/722 - Losses: 0.6003387570381165 + 0.06818463653326035\n",
            "\t- Step 29/722 - Losses: 0.5271093845367432 + 0.04535031318664551\n",
            "\t- Step 30/722 - Losses: 0.5783997774124146 + 0.11876931041479111\n",
            "\t- Step 31/722 - Losses: 0.4985220432281494 + 0.05024284869432449\n",
            "\t- Step 32/722 - Losses: 0.4604603052139282 + 0.03083762526512146\n",
            "\t- Step 33/722 - Losses: 0.4344494938850403 + 0.04509855806827545\n",
            "\t- Step 34/722 - Losses: 0.42560264468193054 + 0.033048707991838455\n",
            "\t- Step 35/722 - Losses: 0.38829517364501953 + 0.02385738492012024\n",
            "\t- Step 36/722 - Losses: 0.3863004744052887 + 0.029565412551164627\n",
            "\t- Step 37/722 - Losses: 0.3534598648548126 + 0.0357636958360672\n",
            "\t- Step 38/722 - Losses: 0.40460753440856934 + 0.09048165380954742\n",
            "\t- Step 39/722 - Losses: 0.35405921936035156 + 0.05080116540193558\n",
            "\t- Step 40/722 - Losses: 0.3926847279071808 + 0.06954477727413177\n",
            "\t- Step 41/722 - Losses: 0.3456507921218872 + 0.0879419669508934\n",
            "\t- Step 42/722 - Losses: 0.32860833406448364 + 0.04550037533044815\n",
            "\t- Step 43/722 - Losses: 0.28992190957069397 + 0.04009729623794556\n",
            "\t- Step 44/722 - Losses: 0.3020271956920624 + 0.0340263769030571\n",
            "\t- Step 45/722 - Losses: 0.30975985527038574 + 0.062134623527526855\n",
            "\t- Step 46/722 - Losses: 0.30565229058265686 + 0.057441674172878265\n",
            "\t- Step 47/722 - Losses: 0.2739737033843994 + 0.035514358431100845\n",
            "\t- Step 48/722 - Losses: 0.2787155508995056 + 0.0477491058409214\n",
            "\t- Step 49/722 - Losses: 0.2740388512611389 + 0.05244115740060806\n",
            "\t- Step 50/722 - Losses: 0.25255101919174194 + 0.03278273344039917\n",
            "\t- Step 51/722 - Losses: 0.24149325489997864 + 0.03811114281415939\n",
            "\t- Step 52/722 - Losses: 0.23480530083179474 + 0.04649324342608452\n",
            "\t- Step 53/722 - Losses: 0.26728805899620056 + 0.09288831055164337\n",
            "\t- Step 54/722 - Losses: 0.2808200418949127 + 0.08046604692935944\n",
            "\t- Step 55/722 - Losses: 0.2690199613571167 + 0.06543488800525665\n",
            "\t- Step 56/722 - Losses: 0.2379082292318344 + 0.05126988887786865\n",
            "\t- Step 57/722 - Losses: 0.258009672164917 + 0.08982748538255692\n",
            "\t- Step 58/722 - Losses: 0.2472013533115387 + 0.07377635687589645\n",
            "\t- Step 59/722 - Losses: 0.21887566149234772 + 0.04951830208301544\n",
            "\t- Step 60/722 - Losses: 0.2812110483646393 + 0.10276761651039124\n",
            "\t- Step 61/722 - Losses: 0.18434718251228333 + 0.030122505500912666\n",
            "\t- Step 62/722 - Losses: 0.1925181746482849 + 0.030794760212302208\n",
            "\t- Step 63/722 - Losses: 0.2659807503223419 + 0.1045946478843689\n",
            "\t- Step 64/722 - Losses: 0.16451475024223328 + 0.01183315459638834\n",
            "\t- Step 65/722 - Losses: 0.21555541455745697 + 0.05423238128423691\n",
            "\t- Step 66/722 - Losses: 0.2089538872241974 + 0.052609823644161224\n",
            "\t- Step 67/722 - Losses: 0.2108139842748642 + 0.07112370431423187\n",
            "\t- Step 68/722 - Losses: 0.1912110447883606 + 0.04665348678827286\n",
            "\t- Step 69/722 - Losses: 0.18743915855884552 + 0.04931376129388809\n",
            "\t- Step 70/722 - Losses: 0.22174663841724396 + 0.0692676305770874\n",
            "\t- Step 71/722 - Losses: 0.15999533236026764 + 0.0374685600399971\n",
            "\t- Step 72/722 - Losses: 0.19660861790180206 + 0.06281749904155731\n",
            "\t- Step 73/722 - Losses: 0.2123315930366516 + 0.07952789962291718\n",
            "\t- Step 74/722 - Losses: 0.27325594425201416 + 0.12395865470170975\n",
            "\t- Step 75/722 - Losses: 0.17363493144512177 + 0.04083152860403061\n",
            "\t- Step 76/722 - Losses: 0.22743995487689972 + 0.07850997895002365\n",
            "\t- Step 77/722 - Losses: 0.1337129771709442 + 0.012394030578434467\n",
            "\t- Step 78/722 - Losses: 0.1757490336894989 + 0.0395163930952549\n",
            "\t- Step 79/722 - Losses: 0.22370193898677826 + 0.07446064800024033\n",
            "\t- Step 80/722 - Losses: 0.20195342600345612 + 0.063076451420784\n",
            "\t- Step 81/722 - Losses: 0.15532265603542328 + 0.044550858438014984\n",
            "\t- Step 82/722 - Losses: 0.1793937087059021 + 0.06897257268428802\n",
            "\t- Step 83/722 - Losses: 0.14654268324375153 + 0.029467038810253143\n",
            "\t- Step 84/722 - Losses: 0.16147570312023163 + 0.03483403101563454\n",
            "\t- Step 85/722 - Losses: 0.1342346966266632 + 0.028787339106202126\n",
            "\t- Step 86/722 - Losses: 0.16939370334148407 + 0.043885648250579834\n",
            "\t- Step 87/722 - Losses: 0.13189254701137543 + 0.03236064314842224\n",
            "\t- Step 88/722 - Losses: 0.18753936886787415 + 0.06516902148723602\n",
            "\t- Step 89/722 - Losses: 0.1778896301984787 + 0.04530410096049309\n",
            "\t- Step 90/722 - Losses: 0.14799055457115173 + 0.04472069442272186\n",
            "\t- Step 91/722 - Losses: 0.14273422956466675 + 0.038957223296165466\n",
            "\t- Step 92/722 - Losses: 0.1865273118019104 + 0.06634114682674408\n",
            "\t- Step 93/722 - Losses: 0.18241305649280548 + 0.07197163254022598\n",
            "\t- Step 94/722 - Losses: 0.13178005814552307 + 0.03067433089017868\n",
            "\t- Step 95/722 - Losses: 0.14847643673419952 + 0.04800865426659584\n",
            "\t- Step 96/722 - Losses: 0.13108231127262115 + 0.0360713005065918\n",
            "\t- Step 97/722 - Losses: 0.14076466858386993 + 0.04209928959608078\n",
            "\t- Step 98/722 - Losses: 0.16505534946918488 + 0.05168091505765915\n",
            "\t- Step 99/722 - Losses: 0.1486770212650299 + 0.05294778198003769\n",
            "\t- Step 100/722 - Losses: 0.10476282238960266 + 0.018250055611133575\n",
            "\t- Step 101/722 - Losses: 0.1711365431547165 + 0.05509098619222641\n",
            "\t- Step 102/722 - Losses: 0.18086840212345123 + 0.06357535719871521\n",
            "\t- Step 103/722 - Losses: 0.10172063857316971 + 0.007900962606072426\n",
            "\t- Step 104/722 - Losses: 0.14129769802093506 + 0.05071885883808136\n",
            "\t- Step 105/722 - Losses: 0.1197681725025177 + 0.03425687924027443\n",
            "\t- Step 106/722 - Losses: 0.1198464035987854 + 0.026479292660951614\n",
            "\t- Step 107/722 - Losses: 0.14182977378368378 + 0.038785602897405624\n",
            "\t- Step 108/722 - Losses: 0.1448991298675537 + 0.038509078323841095\n",
            "\t- Step 109/722 - Losses: 0.15995736420154572 + 0.050976332277059555\n",
            "\t- Step 110/722 - Losses: 0.17262385785579681 + 0.05961073935031891\n",
            "\t- Step 111/722 - Losses: 0.16508394479751587 + 0.07563357800245285\n",
            "\t- Step 112/722 - Losses: 0.17352695763111115 + 0.05421561375260353\n",
            "\t- Step 113/722 - Losses: 0.18703769147396088 + 0.09624958038330078\n",
            "\t- Step 114/722 - Losses: 0.13055503368377686 + 0.05289328843355179\n",
            "\t- Step 115/722 - Losses: 0.12283356487751007 + 0.034965936094522476\n",
            "\t- Step 116/722 - Losses: 0.14181238412857056 + 0.04364564269781113\n",
            "\t- Step 117/722 - Losses: 0.14386525750160217 + 0.05726234242320061\n",
            "\t- Step 118/722 - Losses: 0.18474546074867249 + 0.08052234351634979\n",
            "\t- Step 119/722 - Losses: 0.1385282725095749 + 0.0479409396648407\n",
            "\t- Step 120/722 - Losses: 0.11266840249300003 + 0.030433595180511475\n",
            "\t- Step 121/722 - Losses: 0.1175658255815506 + 0.03652102127671242\n",
            "\t- Step 122/722 - Losses: 0.1764795184135437 + 0.0744420662522316\n",
            "\t- Step 123/722 - Losses: 0.15137596428394318 + 0.0683027058839798\n",
            "\t- Step 124/722 - Losses: 0.14909504354000092 + 0.048636674880981445\n",
            "\t- Step 125/722 - Losses: 0.16382497549057007 + 0.07198259979486465\n",
            "\t- Step 126/722 - Losses: 0.14155696332454681 + 0.04460836946964264\n",
            "\t- Step 127/722 - Losses: 0.10206011682748795 + 0.02570391073822975\n",
            "\t- Step 128/722 - Losses: 0.12541498243808746 + 0.034321703016757965\n",
            "\t- Step 129/722 - Losses: 0.1519065797328949 + 0.06637081503868103\n",
            "\t- Step 130/722 - Losses: 0.15905256569385529 + 0.0650000348687172\n",
            "\t- Step 131/722 - Losses: 0.15930715203285217 + 0.05346060171723366\n",
            "\t- Step 132/722 - Losses: 0.12450653314590454 + 0.04286612197756767\n",
            "\t- Step 133/722 - Losses: 0.18592360615730286 + 0.0898510217666626\n",
            "\t- Step 134/722 - Losses: 0.11660492420196533 + 0.03824738413095474\n",
            "\t- Step 135/722 - Losses: 0.14260251820087433 + 0.05420303717255592\n",
            "\t- Step 136/722 - Losses: 0.13765248656272888 + 0.04588647186756134\n",
            "\t- Step 137/722 - Losses: 0.08034884184598923 + 0.016719715669751167\n",
            "\t- Step 138/722 - Losses: 0.13757354021072388 + 0.06299576163291931\n",
            "\t- Step 139/722 - Losses: 0.14526240527629852 + 0.06686464697122574\n",
            "\t- Step 140/722 - Losses: 0.11042684316635132 + 0.03394068405032158\n",
            "\t- Step 141/722 - Losses: 0.12814734876155853 + 0.049516644328832626\n",
            "\t- Step 142/722 - Losses: 0.15563008189201355 + 0.06975427269935608\n",
            "\t- Step 143/722 - Losses: 0.12528188526630402 + 0.03623353689908981\n",
            "\t- Step 144/722 - Losses: 0.1171213835477829 + 0.03346481919288635\n",
            "\t- Step 145/722 - Losses: 0.10160200297832489 + 0.02574766054749489\n",
            "\t- Step 146/722 - Losses: 0.1311212182044983 + 0.057583943009376526\n",
            "\t- Step 147/722 - Losses: 0.14525051414966583 + 0.059619754552841187\n",
            "\t- Step 148/722 - Losses: 0.08896200358867645 + 0.024427758529782295\n",
            "\t- Step 149/722 - Losses: 0.132313534617424 + 0.050541095435619354\n",
            "\t- Step 150/722 - Losses: 0.08417914807796478 + 0.01984204724431038\n",
            "\t- Step 151/722 - Losses: 0.12267565727233887 + 0.047613877803087234\n",
            "\t- Step 152/722 - Losses: 0.08959095180034637 + 0.03417850285768509\n",
            "\t- Step 153/722 - Losses: 0.09421587735414505 + 0.029524272307753563\n",
            "\t- Step 154/722 - Losses: 0.11856018751859665 + 0.05058865249156952\n",
            "\t- Step 155/722 - Losses: 0.13005222380161285 + 0.04658745974302292\n",
            "\t- Step 156/722 - Losses: 0.10163373500108719 + 0.04538022354245186\n",
            "\t- Step 157/722 - Losses: 0.0968470349907875 + 0.03162223473191261\n",
            "\t- Step 158/722 - Losses: 0.15752610564231873 + 0.07488542795181274\n",
            "\t- Step 159/722 - Losses: 0.15515080094337463 + 0.07331731170415878\n",
            "\t- Step 160/722 - Losses: 0.09358543157577515 + 0.030786652117967606\n",
            "\t- Step 161/722 - Losses: 0.10142924636602402 + 0.03386031091213226\n",
            "\t- Step 162/722 - Losses: 0.11674418300390244 + 0.04022040590643883\n",
            "\t- Step 163/722 - Losses: 0.1208062544465065 + 0.05041500926017761\n",
            "\t- Step 164/722 - Losses: 0.20102854073047638 + 0.09686729311943054\n",
            "\t- Step 165/722 - Losses: 0.17956040799617767 + 0.054669834673404694\n",
            "\t- Step 166/722 - Losses: 0.16120514273643494 + 0.07522712647914886\n",
            "\t- Step 167/722 - Losses: 0.11703594028949738 + 0.0491204708814621\n",
            "\t- Step 168/722 - Losses: 0.1123180240392685 + 0.037743568420410156\n",
            "\t- Step 169/722 - Losses: 0.1364821195602417 + 0.05444038659334183\n",
            "\t- Step 170/722 - Losses: 0.1617496758699417 + 0.0760694071650505\n",
            "\t- Step 171/722 - Losses: 0.1092664897441864 + 0.04416870325803757\n",
            "\t- Step 172/722 - Losses: 0.11849355697631836 + 0.035751890391111374\n",
            "\t- Step 173/722 - Losses: 0.07749767601490021 + 0.016767092049121857\n",
            "\t- Step 174/722 - Losses: 0.12796643376350403 + 0.04704158008098602\n",
            "\t- Step 175/722 - Losses: 0.13096493482589722 + 0.05593816190958023\n",
            "\t- Step 176/722 - Losses: 0.1360550969839096 + 0.05010122433304787\n",
            "\t- Step 177/722 - Losses: 0.12684941291809082 + 0.0455281138420105\n",
            "\t- Step 178/722 - Losses: 0.1461913138628006 + 0.05837511643767357\n",
            "\t- Step 179/722 - Losses: 0.06364397704601288 + 0.017926741391420364\n",
            "\t- Step 180/722 - Losses: 0.14707253873348236 + 0.07077094167470932\n",
            "\t- Step 181/722 - Losses: 0.14582964777946472 + 0.08837906271219254\n",
            "\t- Step 182/722 - Losses: 0.15278159081935883 + 0.07791205495595932\n",
            "\t- Step 183/722 - Losses: 0.10429040342569351 + 0.03679011017084122\n",
            "\t- Step 184/722 - Losses: 0.2141415923833847 + 0.111616350710392\n",
            "\t- Step 185/722 - Losses: 0.09536464512348175 + 0.03467433154582977\n",
            "\t- Step 186/722 - Losses: 0.10875783115625381 + 0.042997878044843674\n",
            "\t- Step 187/722 - Losses: 0.07857473939657211 + 0.025267211720347404\n",
            "\t- Step 188/722 - Losses: 0.1759253442287445 + 0.09339800477027893\n",
            "\t- Step 189/722 - Losses: 0.1079326942563057 + 0.035766907036304474\n",
            "\t- Step 190/722 - Losses: 0.10110455751419067 + 0.03849891200661659\n",
            "\t- Step 191/722 - Losses: 0.1698298454284668 + 0.06956997513771057\n",
            "\t- Step 192/722 - Losses: 0.14566926658153534 + 0.062481872737407684\n",
            "\t- Step 193/722 - Losses: 0.14827799797058105 + 0.06793287396430969\n",
            "\t- Step 194/722 - Losses: 0.17661038041114807 + 0.08836740255355835\n",
            "\t- Step 195/722 - Losses: 0.11434324830770493 + 0.044361844658851624\n",
            "\t- Step 196/722 - Losses: 0.08188095688819885 + 0.02203163132071495\n",
            "\t- Step 197/722 - Losses: 0.23275423049926758 + 0.13517288863658905\n",
            "\t- Step 198/722 - Losses: 0.13394920527935028 + 0.06989751756191254\n",
            "\t- Step 199/722 - Losses: 0.11717474460601807 + 0.04601234570145607\n",
            "\t- Step 200/722 - Losses: 0.13142108917236328 + 0.04668101668357849\n",
            "\t- Step 201/722 - Losses: 0.07848481088876724 + 0.029602566733956337\n",
            "\t- Step 202/722 - Losses: 0.15441836416721344 + 0.0758184865117073\n",
            "\t- Step 203/722 - Losses: 0.08042226731777191 + 0.029354896396398544\n",
            "\t- Step 204/722 - Losses: 0.08578583598136902 + 0.03448881208896637\n",
            "\t- Step 205/722 - Losses: 0.08119651675224304 + 0.025817058980464935\n",
            "\t- Step 206/722 - Losses: 0.10735210031270981 + 0.04047543928027153\n",
            "\t- Step 207/722 - Losses: 0.15063048899173737 + 0.06296712905168533\n",
            "\t- Step 208/722 - Losses: 0.11168470978736877 + 0.05727595090866089\n",
            "\t- Step 209/722 - Losses: 0.13271808624267578 + 0.06611225008964539\n",
            "\t- Step 210/722 - Losses: 0.18476063013076782 + 0.09370477497577667\n",
            "\t- Step 211/722 - Losses: 0.09108085930347443 + 0.03019067272543907\n",
            "\t- Step 212/722 - Losses: 0.08443285524845123 + 0.032464295625686646\n",
            "\t- Step 213/722 - Losses: 0.09413288533687592 + 0.03897102177143097\n",
            "\t- Step 214/722 - Losses: 0.14897072315216064 + 0.06285680085420609\n",
            "\t- Step 215/722 - Losses: 0.17250990867614746 + 0.07671518623828888\n",
            "\t- Step 216/722 - Losses: 0.13298150897026062 + 0.05727120488882065\n",
            "\t- Step 217/722 - Losses: 0.10850558429956436 + 0.04112241417169571\n",
            "\t- Step 218/722 - Losses: 0.12852409482002258 + 0.05547985062003136\n",
            "\t- Step 219/722 - Losses: 0.0994940847158432 + 0.0410018153488636\n",
            "\t- Step 220/722 - Losses: 0.0950552448630333 + 0.02981042116880417\n",
            "\t- Step 221/722 - Losses: 0.12230134755373001 + 0.05713401734828949\n",
            "\t- Step 222/722 - Losses: 0.1195482611656189 + 0.05101674050092697\n",
            "\t- Step 223/722 - Losses: 0.06510423123836517 + 0.015447654761373997\n",
            "\t- Step 224/722 - Losses: 0.11426787078380585 + 0.04170871526002884\n",
            "\t- Step 225/722 - Losses: 0.12197895348072052 + 0.0535973459482193\n",
            "\t- Step 226/722 - Losses: 0.1270342469215393 + 0.06916078180074692\n",
            "\t- Step 227/722 - Losses: 0.1690802425146103 + 0.08373052626848221\n",
            "\t- Step 228/722 - Losses: 0.09616836160421371 + 0.04283253103494644\n",
            "\t- Step 229/722 - Losses: 0.12457703053951263 + 0.05430041253566742\n",
            "\t- Step 230/722 - Losses: 0.11343628168106079 + 0.0469721257686615\n",
            "\t- Step 231/722 - Losses: 0.14626646041870117 + 0.07267698645591736\n",
            "\t- Step 232/722 - Losses: 0.10104251652956009 + 0.04233087971806526\n",
            "\t- Step 233/722 - Losses: 0.0923716202378273 + 0.03860431909561157\n",
            "\t- Step 234/722 - Losses: 0.11523126810789108 + 0.052371494472026825\n",
            "\t- Step 235/722 - Losses: 0.12866142392158508 + 0.056585103273391724\n",
            "\t- Step 236/722 - Losses: 0.07854821532964706 + 0.02402813732624054\n",
            "\t- Step 237/722 - Losses: 0.10487448424100876 + 0.04454346001148224\n",
            "\t- Step 238/722 - Losses: 0.17476291954517365 + 0.08429981768131256\n",
            "\t- Step 239/722 - Losses: 0.13328324258327484 + 0.05809272453188896\n",
            "\t- Step 240/722 - Losses: 0.0954558476805687 + 0.036661893129348755\n",
            "\t- Step 241/722 - Losses: 0.049172721803188324 + 0.008974684402346611\n",
            "\t- Step 242/722 - Losses: 0.10462507605552673 + 0.041451454162597656\n",
            "\t- Step 243/722 - Losses: 0.1362077295780182 + 0.06280427426099777\n",
            "\t- Step 244/722 - Losses: 0.08254952728748322 + 0.0293281227350235\n",
            "\t- Step 245/722 - Losses: 0.1037580668926239 + 0.05078204721212387\n",
            "\t- Step 246/722 - Losses: 0.11308812350034714 + 0.04915216565132141\n",
            "\t- Step 247/722 - Losses: 0.11741119623184204 + 0.04630153626203537\n",
            "\t- Step 248/722 - Losses: 0.10282523930072784 + 0.02976818010210991\n",
            "\t- Step 249/722 - Losses: 0.14657241106033325 + 0.07536110281944275\n",
            "\t- Step 250/722 - Losses: 0.10824792087078094 + 0.037628743797540665\n",
            "\t- Step 251/722 - Losses: 0.12130710482597351 + 0.060964446514844894\n",
            "\t- Step 252/722 - Losses: 0.14083930850028992 + 0.06339322775602341\n",
            "\t- Step 253/722 - Losses: 0.11024722456932068 + 0.05698006972670555\n",
            "\t- Step 254/722 - Losses: 0.07283799350261688 + 0.023147903382778168\n",
            "\t- Step 255/722 - Losses: 0.11139238625764847 + 0.03719952329993248\n",
            "\t- Step 256/722 - Losses: 0.19259139895439148 + 0.09342886507511139\n",
            "\t- Step 257/722 - Losses: 0.15740364789962769 + 0.07491686940193176\n",
            "\t- Step 258/722 - Losses: 0.1012045368552208 + 0.049302421510219574\n",
            "\t- Step 259/722 - Losses: 0.12738484144210815 + 0.050363458693027496\n",
            "\t- Step 260/722 - Losses: 0.15084685385227203 + 0.06881794333457947\n",
            "\t- Step 261/722 - Losses: 0.13258858025074005 + 0.06345464289188385\n",
            "\t- Step 262/722 - Losses: 0.09081275761127472 + 0.03903738409280777\n",
            "\t- Step 263/722 - Losses: 0.07783502340316772 + 0.02979842945933342\n",
            "\t- Step 264/722 - Losses: 0.12196914851665497 + 0.05232171714305878\n",
            "\t- Step 265/722 - Losses: 0.10977079719305038 + 0.0412844642996788\n",
            "\t- Step 266/722 - Losses: 0.1420103758573532 + 0.06905853748321533\n",
            "\t- Step 267/722 - Losses: 0.1305077224969864 + 0.05874018371105194\n",
            "\t- Step 268/722 - Losses: 0.1310669481754303 + 0.06334561109542847\n",
            "\t- Step 269/722 - Losses: 0.16790586709976196 + 0.08967973291873932\n",
            "\t- Step 270/722 - Losses: 0.15180061757564545 + 0.05968310683965683\n",
            "\t- Step 271/722 - Losses: 0.12406416982412338 + 0.05354350060224533\n",
            "\t- Step 272/722 - Losses: 0.09179419279098511 + 0.041279640048742294\n",
            "\t- Step 273/722 - Losses: 0.13125523924827576 + 0.057822488248348236\n",
            "\t- Step 274/722 - Losses: 0.08340204507112503 + 0.03686780110001564\n",
            "\t- Step 275/722 - Losses: 0.09810158610343933 + 0.04107208549976349\n",
            "\t- Step 276/722 - Losses: 0.16672582924365997 + 0.07472358644008636\n",
            "\t- Step 277/722 - Losses: 0.13222633302211761 + 0.06727644801139832\n",
            "\t- Step 278/722 - Losses: 0.09059172868728638 + 0.031624771654605865\n",
            "\t- Step 279/722 - Losses: 0.12473110854625702 + 0.05590565875172615\n",
            "\t- Step 280/722 - Losses: 0.0899055078625679 + 0.03910282999277115\n",
            "\t- Step 281/722 - Losses: 0.056502800434827805 + 0.014166686683893204\n",
            "\t- Step 282/722 - Losses: 0.05050386115908623 + 0.015653248876333237\n",
            "\t- Step 283/722 - Losses: 0.09686609357595444 + 0.03420395776629448\n",
            "\t- Step 284/722 - Losses: 0.0860905796289444 + 0.03523694723844528\n",
            "\t- Step 285/722 - Losses: 0.08554433286190033 + 0.030834266915917397\n",
            "\t- Step 286/722 - Losses: 0.13958026468753815 + 0.0652024894952774\n",
            "\t- Step 287/722 - Losses: 0.16585813462734222 + 0.08498525619506836\n",
            "\t- Step 288/722 - Losses: 0.09217898547649384 + 0.043467871844768524\n",
            "\t- Step 289/722 - Losses: 0.07944009453058243 + 0.028012806549668312\n",
            "\t- Step 290/722 - Losses: 0.13217037916183472 + 0.06003105267882347\n",
            "\t- Step 291/722 - Losses: 0.13688167929649353 + 0.06688064336776733\n",
            "\t- Step 292/722 - Losses: 0.09522734582424164 + 0.037189871072769165\n",
            "\t- Step 293/722 - Losses: 0.08365184813737869 + 0.03305505961179733\n",
            "\t- Step 294/722 - Losses: 0.10370573401451111 + 0.039543479681015015\n",
            "\t- Step 295/722 - Losses: 0.14834243059158325 + 0.0684526190161705\n",
            "\t- Step 296/722 - Losses: 0.057855941355228424 + 0.018933245912194252\n",
            "\t- Step 297/722 - Losses: 0.10849744826555252 + 0.05682199448347092\n",
            "\t- Step 298/722 - Losses: 0.16858628392219543 + 0.08834979683160782\n",
            "\t- Step 299/722 - Losses: 0.19200290739536285 + 0.09529498219490051\n",
            "\t- Step 300/722 - Losses: 0.08003876358270645 + 0.02909904345870018\n",
            "\t- Step 301/722 - Losses: 0.09570226818323135 + 0.04206494614481926\n",
            "\t- Step 302/722 - Losses: 0.16066555678844452 + 0.08736857771873474\n",
            "\t- Step 303/722 - Losses: 0.12098248302936554 + 0.055517591536045074\n",
            "\t- Step 304/722 - Losses: 0.12696313858032227 + 0.05771335959434509\n",
            "\t- Step 305/722 - Losses: 0.058769937604665756 + 0.021129712462425232\n",
            "\t- Step 306/722 - Losses: 0.17700515687465668 + 0.09107154607772827\n",
            "\t- Step 307/722 - Losses: 0.13690586388111115 + 0.07116106152534485\n",
            "\t- Step 308/722 - Losses: 0.1266712099313736 + 0.055232152342796326\n",
            "\t- Step 309/722 - Losses: 0.10203876346349716 + 0.03991636633872986\n",
            "\t- Step 310/722 - Losses: 0.06373096257448196 + 0.021161939948797226\n",
            "\t- Step 311/722 - Losses: 0.1304904967546463 + 0.06963872909545898\n",
            "\t- Step 312/722 - Losses: 0.07199819386005402 + 0.02745060995221138\n",
            "\t- Step 313/722 - Losses: 0.09980356693267822 + 0.04295303300023079\n",
            "\t- Step 314/722 - Losses: 0.12960503995418549 + 0.06865725666284561\n",
            "\t- Step 315/722 - Losses: 0.11337596923112869 + 0.057135120034217834\n",
            "\t- Step 316/722 - Losses: 0.1668470948934555 + 0.07625138759613037\n",
            "\t- Step 317/722 - Losses: 0.11190441995859146 + 0.044878698885440826\n",
            "\t- Step 318/722 - Losses: 0.13084402680397034 + 0.06556335091590881\n",
            "\t- Step 319/722 - Losses: 0.10262130945920944 + 0.04295816272497177\n",
            "\t- Step 320/722 - Losses: 0.0903598815202713 + 0.02851978689432144\n",
            "\t- Step 321/722 - Losses: 0.17941555380821228 + 0.08654330670833588\n",
            "\t- Step 322/722 - Losses: 0.06352932751178741 + 0.01585155352950096\n",
            "\t- Step 323/722 - Losses: 0.11293528228998184 + 0.04306842386722565\n",
            "\t- Step 324/722 - Losses: 0.14850519597530365 + 0.0705719143152237\n",
            "\t- Step 325/722 - Losses: 0.12271443754434586 + 0.050088100135326385\n",
            "\t- Step 326/722 - Losses: 0.12092503905296326 + 0.046483587473630905\n",
            "\t- Step 327/722 - Losses: 0.17247359454631805 + 0.08654895424842834\n",
            "\t- Step 328/722 - Losses: 0.09795218706130981 + 0.0401851087808609\n",
            "\t- Step 329/722 - Losses: 0.08814601600170135 + 0.04418819770216942\n",
            "\t- Step 330/722 - Losses: 0.10336916148662567 + 0.05149494484066963\n",
            "\t- Step 331/722 - Losses: 0.12539294362068176 + 0.05824346840381622\n",
            "\t- Step 332/722 - Losses: 0.13484501838684082 + 0.06135886162519455\n",
            "\t- Step 333/722 - Losses: 0.1370963305234909 + 0.05536893010139465\n",
            "\t- Step 334/722 - Losses: 0.09949605911970139 + 0.04587801545858383\n",
            "\t- Step 335/722 - Losses: 0.12154041230678558 + 0.06354422122240067\n",
            "\t- Step 336/722 - Losses: 0.08250685781240463 + 0.03921826183795929\n",
            "\t- Step 337/722 - Losses: 0.10430929064750671 + 0.04482642933726311\n",
            "\t- Step 338/722 - Losses: 0.12165270745754242 + 0.05453033745288849\n",
            "\t- Step 339/722 - Losses: 0.09863240271806717 + 0.047040797770023346\n",
            "\t- Step 340/722 - Losses: 0.10429772734642029 + 0.04781380295753479\n",
            "\t- Step 341/722 - Losses: 0.0918542742729187 + 0.04105595499277115\n",
            "\t- Step 342/722 - Losses: 0.12357484549283981 + 0.05714261904358864\n",
            "\t- Step 343/722 - Losses: 0.10527513176202774 + 0.04019841551780701\n",
            "\t- Step 344/722 - Losses: 0.12012635916471481 + 0.06119775027036667\n",
            "\t- Step 345/722 - Losses: 0.07490652799606323 + 0.03168746829032898\n",
            "\t- Step 346/722 - Losses: 0.0915161743760109 + 0.03976291045546532\n",
            "\t- Step 347/722 - Losses: 0.14114239811897278 + 0.06421394646167755\n",
            "\t- Step 348/722 - Losses: 0.09351812303066254 + 0.042756736278533936\n",
            "\t- Step 349/722 - Losses: 0.1261078268289566 + 0.061615295708179474\n",
            "\t- Step 350/722 - Losses: 0.14123474061489105 + 0.06229488179087639\n",
            "\t- Step 351/722 - Losses: 0.14262095093727112 + 0.054097436368465424\n",
            "\t- Step 352/722 - Losses: 0.07525919377803802 + 0.023762155324220657\n",
            "\t- Step 353/722 - Losses: 0.10241055488586426 + 0.045941635966300964\n",
            "\t- Step 354/722 - Losses: 0.1299566626548767 + 0.05235572159290314\n",
            "\t- Step 355/722 - Losses: 0.12682569026947021 + 0.060094863176345825\n",
            "\t- Step 356/722 - Losses: 0.08599388599395752 + 0.034544508904218674\n",
            "\t- Step 357/722 - Losses: 0.09800201654434204 + 0.03824188560247421\n",
            "\t- Step 358/722 - Losses: 0.15912221372127533 + 0.0816606730222702\n",
            "\t- Step 359/722 - Losses: 0.09577180445194244 + 0.04131494462490082\n",
            "\t- Step 360/722 - Losses: 0.14028963446617126 + 0.061651650816202164\n",
            "\t- Step 361/722 - Losses: 0.11068566143512726 + 0.04547327756881714\n",
            "\t- Step 362/722 - Losses: 0.17386531829833984 + 0.08631586283445358\n",
            "\t- Step 363/722 - Losses: 0.09625642746686935 + 0.04964233934879303\n",
            "\t- Step 364/722 - Losses: 0.11814403533935547 + 0.057992123067379\n",
            "\t- Step 365/722 - Losses: 0.08842411637306213 + 0.03378330171108246\n",
            "\t- Step 366/722 - Losses: 0.11293062567710876 + 0.03615092486143112\n",
            "\t- Step 367/722 - Losses: 0.19349327683448792 + 0.09396949410438538\n",
            "\t- Step 368/722 - Losses: 0.14449617266654968 + 0.06428615748882294\n",
            "\t- Step 369/722 - Losses: 0.06387166678905487 + 0.021679915487766266\n",
            "\t- Step 370/722 - Losses: 0.0993775874376297 + 0.03437071666121483\n",
            "\t- Step 371/722 - Losses: 0.22094941139221191 + 0.10699589550495148\n",
            "\t- Step 372/722 - Losses: 0.10117097198963165 + 0.04933091253042221\n",
            "\t- Step 373/722 - Losses: 0.0922316312789917 + 0.04219342768192291\n",
            "\t- Step 374/722 - Losses: 0.13298696279525757 + 0.07738438248634338\n",
            "\t- Step 375/722 - Losses: 0.0727088451385498 + 0.03445449471473694\n",
            "\t- Step 376/722 - Losses: 0.08562369644641876 + 0.027683977037668228\n",
            "\t- Step 377/722 - Losses: 0.09343203157186508 + 0.04246988892555237\n",
            "\t- Step 378/722 - Losses: 0.14647185802459717 + 0.0788007602095604\n",
            "\t- Step 379/722 - Losses: 0.08828484266996384 + 0.0388544499874115\n",
            "\t- Step 380/722 - Losses: 0.09974795579910278 + 0.04332096874713898\n",
            "\t- Step 381/722 - Losses: 0.08041800558567047 + 0.03592576086521149\n",
            "\t- Step 382/722 - Losses: 0.11852287501096725 + 0.050610870122909546\n",
            "\t- Step 383/722 - Losses: 0.11696809530258179 + 0.05217282474040985\n",
            "\t- Step 384/722 - Losses: 0.07901734858751297 + 0.03298243135213852\n",
            "\t- Step 385/722 - Losses: 0.09600429981946945 + 0.03694292530417442\n",
            "\t- Step 386/722 - Losses: 0.11683157086372375 + 0.05102306231856346\n",
            "\t- Step 387/722 - Losses: 0.1259954422712326 + 0.0594601146876812\n",
            "\t- Step 388/722 - Losses: 0.083523690700531 + 0.022417958825826645\n",
            "\t- Step 389/722 - Losses: 0.15305666625499725 + 0.06825703382492065\n",
            "\t- Step 390/722 - Losses: 0.10436683893203735 + 0.04592692106962204\n",
            "\t- Step 391/722 - Losses: 0.07370080053806305 + 0.029152533039450645\n",
            "\t- Step 392/722 - Losses: 0.13228105008602142 + 0.05727902054786682\n",
            "\t- Step 393/722 - Losses: 0.06367146223783493 + 0.02061021700501442\n",
            "\t- Step 394/722 - Losses: 0.06985053420066833 + 0.024967070668935776\n",
            "\t- Step 395/722 - Losses: 0.08293922245502472 + 0.03125035762786865\n",
            "\t- Step 396/722 - Losses: 0.17024198174476624 + 0.07405109703540802\n",
            "\t- Step 397/722 - Losses: 0.11226962506771088 + 0.052808135747909546\n",
            "\t- Step 398/722 - Losses: 0.09668247401714325 + 0.04142835736274719\n",
            "\t- Step 399/722 - Losses: 0.09525088965892792 + 0.03990005701780319\n",
            "\t- Step 400/722 - Losses: 0.08152271807193756 + 0.03535022214055061\n",
            "\t- Step 401/722 - Losses: 0.08428831398487091 + 0.03430505841970444\n",
            "\t- Step 402/722 - Losses: 0.11517737060785294 + 0.05634423717856407\n",
            "\t- Step 403/722 - Losses: 0.16525837779045105 + 0.07446631789207458\n",
            "\t- Step 404/722 - Losses: 0.07693610340356827 + 0.02123420126736164\n",
            "\t- Step 405/722 - Losses: 0.08530823886394501 + 0.032747991383075714\n",
            "\t- Step 406/722 - Losses: 0.10433738678693771 + 0.048824384808540344\n",
            "\t- Step 407/722 - Losses: 0.07010842859745026 + 0.03279155492782593\n",
            "\t- Step 408/722 - Losses: 0.17482128739356995 + 0.08571058511734009\n",
            "\t- Step 409/722 - Losses: 0.08142959326505661 + 0.03591100126504898\n",
            "\t- Step 410/722 - Losses: 0.10736173391342163 + 0.04633340239524841\n",
            "\t- Step 411/722 - Losses: 0.11237198114395142 + 0.04679775983095169\n",
            "\t- Step 412/722 - Losses: 0.1284150779247284 + 0.05983313173055649\n",
            "\t- Step 413/722 - Losses: 0.08143826574087143 + 0.03749197721481323\n",
            "\t- Step 414/722 - Losses: 0.13713793456554413 + 0.07320718467235565\n",
            "\t- Step 415/722 - Losses: 0.11652214080095291 + 0.06093629449605942\n",
            "\t- Step 416/722 - Losses: 0.07825665920972824 + 0.028609510511159897\n",
            "\t- Step 417/722 - Losses: 0.04919158294796944 + 0.0141036007553339\n",
            "\t- Step 418/722 - Losses: 0.16951894760131836 + 0.08262856304645538\n",
            "\t- Step 419/722 - Losses: 0.09631147980690002 + 0.05488130450248718\n",
            "\t- Step 420/722 - Losses: 0.19576092064380646 + 0.10631988197565079\n",
            "\t- Step 421/722 - Losses: 0.09676764905452728 + 0.04382175952196121\n",
            "\t- Step 422/722 - Losses: 0.13659939169883728 + 0.06368343532085419\n",
            "\t- Step 423/722 - Losses: 0.049193061888217926 + 0.01902296021580696\n",
            "\t- Step 424/722 - Losses: 0.05761384963989258 + 0.013455424457788467\n",
            "\t- Step 425/722 - Losses: 0.1288490742444992 + 0.06853312253952026\n",
            "\t- Step 426/722 - Losses: 0.15102709829807281 + 0.07772868126630783\n",
            "\t- Step 427/722 - Losses: 0.10265906155109406 + 0.04696829617023468\n",
            "\t- Step 428/722 - Losses: 0.16708135604858398 + 0.08245551586151123\n",
            "\t- Step 429/722 - Losses: 0.1002800315618515 + 0.056368861347436905\n",
            "\t- Step 430/722 - Losses: 0.11062642186880112 + 0.05602382868528366\n",
            "\t- Step 431/722 - Losses: 0.14805546402931213 + 0.07132159173488617\n",
            "\t- Step 432/722 - Losses: 0.1732366681098938 + 0.0719480887055397\n",
            "\t- Step 433/722 - Losses: 0.10480475425720215 + 0.042080964893102646\n",
            "\t- Step 434/722 - Losses: 0.08183147758245468 + 0.03532809764146805\n",
            "\t- Step 435/722 - Losses: 0.1262950599193573 + 0.057688795030117035\n",
            "\t- Step 436/722 - Losses: 0.07210293412208557 + 0.03201077878475189\n",
            "\t- Step 437/722 - Losses: 0.08210623264312744 + 0.034342579543590546\n",
            "\t- Step 438/722 - Losses: 0.09517990797758102 + 0.053196169435977936\n",
            "\t- Step 439/722 - Losses: 0.06596431881189346 + 0.024518974125385284\n",
            "\t- Step 440/722 - Losses: 0.12009052187204361 + 0.05276819318532944\n",
            "\t- Step 441/722 - Losses: 0.1388573944568634 + 0.06454773247241974\n",
            "\t- Step 442/722 - Losses: 0.09009867906570435 + 0.029707759618759155\n",
            "\t- Step 443/722 - Losses: 0.1288185715675354 + 0.061226315796375275\n",
            "\t- Step 444/722 - Losses: 0.08929899334907532 + 0.03859364241361618\n",
            "\t- Step 445/722 - Losses: 0.0990549623966217 + 0.050405193120241165\n",
            "\t- Step 446/722 - Losses: 0.06500054895877838 + 0.02543516457080841\n",
            "\t- Step 447/722 - Losses: 0.10569898039102554 + 0.04303593933582306\n",
            "\t- Step 448/722 - Losses: 0.12889163196086884 + 0.05086052045226097\n",
            "\t- Step 449/722 - Losses: 0.10438615083694458 + 0.05157015472650528\n",
            "\t- Step 450/722 - Losses: 0.08145259320735931 + 0.035332269966602325\n",
            "\t- Step 451/722 - Losses: 0.12819895148277283 + 0.06579142808914185\n",
            "\t- Step 452/722 - Losses: 0.1474425047636032 + 0.07536295056343079\n",
            "\t- Step 453/722 - Losses: 0.0946514904499054 + 0.0492352694272995\n",
            "\t- Step 454/722 - Losses: 0.13751131296157837 + 0.07711287587881088\n",
            "\t- Step 455/722 - Losses: 0.14513976871967316 + 0.07605429738759995\n",
            "\t- Step 456/722 - Losses: 0.10359100997447968 + 0.04090569168329239\n",
            "\t- Step 457/722 - Losses: 0.10143826901912689 + 0.04260041192173958\n",
            "\t- Step 458/722 - Losses: 0.11870065331459045 + 0.04598473012447357\n",
            "\t- Step 459/722 - Losses: 0.0900031328201294 + 0.0289408378303051\n",
            "\t- Step 460/722 - Losses: 0.10622576624155045 + 0.05839076638221741\n",
            "\t- Step 461/722 - Losses: 0.10125632584095001 + 0.03956661373376846\n",
            "\t- Step 462/722 - Losses: 0.07243800908327103 + 0.02834673784673214\n",
            "\t- Step 463/722 - Losses: 0.14553318917751312 + 0.07132013142108917\n",
            "\t- Step 464/722 - Losses: 0.13182011246681213 + 0.059752315282821655\n",
            "\t- Step 465/722 - Losses: 0.10642485320568085 + 0.05812884867191315\n",
            "\t- Step 466/722 - Losses: 0.10197723656892776 + 0.041480571031570435\n",
            "\t- Step 467/722 - Losses: 0.1609216034412384 + 0.0700862854719162\n",
            "\t- Step 468/722 - Losses: 0.17646627128124237 + 0.08715187013149261\n",
            "\t- Step 469/722 - Losses: 0.09470134973526001 + 0.044878870248794556\n",
            "\t- Step 470/722 - Losses: 0.0977652445435524 + 0.036022916436195374\n",
            "\t- Step 471/722 - Losses: 0.07693154364824295 + 0.03248471021652222\n",
            "\t- Step 472/722 - Losses: 0.11538773775100708 + 0.05825009196996689\n",
            "\t- Step 473/722 - Losses: 0.12862944602966309 + 0.05797523260116577\n",
            "\t- Step 474/722 - Losses: 0.09555693715810776 + 0.04232232645153999\n",
            "\t- Step 475/722 - Losses: 0.060503602027893066 + 0.018493855372071266\n",
            "\t- Step 476/722 - Losses: 0.12313457578420639 + 0.05334721505641937\n",
            "\t- Step 477/722 - Losses: 0.08732179552316666 + 0.04665091633796692\n",
            "\t- Step 478/722 - Losses: 0.07260298728942871 + 0.03814711421728134\n",
            "\t- Step 479/722 - Losses: 0.11830245703458786 + 0.051879432052373886\n",
            "\t- Step 480/722 - Losses: 0.11149371415376663 + 0.058270178735256195\n",
            "\t- Step 481/722 - Losses: 0.11364421248435974 + 0.0654938668012619\n",
            "\t- Step 482/722 - Losses: 0.06140468642115593 + 0.02180178090929985\n",
            "\t- Step 483/722 - Losses: 0.07160554081201553 + 0.032694656401872635\n",
            "\t- Step 484/722 - Losses: 0.07616747170686722 + 0.027546297758817673\n",
            "\t- Step 485/722 - Losses: 0.12874242663383484 + 0.05948162078857422\n",
            "\t- Step 486/722 - Losses: 0.09301874041557312 + 0.044531501829624176\n",
            "\t- Step 487/722 - Losses: 0.07763898372650146 + 0.03676474094390869\n",
            "\t- Step 488/722 - Losses: 0.06676147878170013 + 0.03094271756708622\n",
            "\t- Step 489/722 - Losses: 0.08510585129261017 + 0.03739313781261444\n",
            "\t- Step 490/722 - Losses: 0.053953565657138824 + 0.022354047745466232\n",
            "\t- Step 491/722 - Losses: 0.05040168762207031 + 0.015313460491597652\n",
            "\t- Step 492/722 - Losses: 0.09318242222070694 + 0.03711578994989395\n",
            "\t- Step 493/722 - Losses: 0.08425916731357574 + 0.04152686521410942\n",
            "\t- Step 494/722 - Losses: 0.11954395473003387 + 0.055608902126550674\n",
            "\t- Step 495/722 - Losses: 0.08745712041854858 + 0.04709523916244507\n",
            "\t- Step 496/722 - Losses: 0.07252808660268784 + 0.030061550438404083\n",
            "\t- Step 497/722 - Losses: 0.1242913082242012 + 0.05939200893044472\n",
            "\t- Step 498/722 - Losses: 0.09178413450717926 + 0.045428257435560226\n",
            "\t- Step 499/722 - Losses: 0.10207898914813995 + 0.047960974276065826\n",
            "\t- Step 500/722 - Losses: 0.13041475415229797 + 0.05335589870810509\n",
            "\t- Step 501/722 - Losses: 0.11628518998622894 + 0.05013396590948105\n",
            "\t- Step 502/722 - Losses: 0.12565316259860992 + 0.054711297154426575\n",
            "\t- Step 503/722 - Losses: 0.09354893118143082 + 0.03940145671367645\n",
            "\t- Step 504/722 - Losses: 0.08562004566192627 + 0.034482866525650024\n",
            "\t- Step 505/722 - Losses: 0.07600373029708862 + 0.030808430165052414\n",
            "\t- Step 506/722 - Losses: 0.08996725082397461 + 0.04014943167567253\n",
            "\t- Step 507/722 - Losses: 0.0887010470032692 + 0.04464486986398697\n",
            "\t- Step 508/722 - Losses: 0.11694730818271637 + 0.05289922654628754\n",
            "\t- Step 509/722 - Losses: 0.13031573593616486 + 0.06731857359409332\n",
            "\t- Step 510/722 - Losses: 0.08113458752632141 + 0.04202825203537941\n",
            "\t- Step 511/722 - Losses: 0.1224302351474762 + 0.046150244772434235\n",
            "\t- Step 512/722 - Losses: 0.07348637282848358 + 0.02940506674349308\n",
            "\t- Step 513/722 - Losses: 0.11164829879999161 + 0.04575129598379135\n",
            "\t- Step 514/722 - Losses: 0.08446849882602692 + 0.036407262086868286\n",
            "\t- Step 515/722 - Losses: 0.14689886569976807 + 0.07297395914793015\n",
            "\t- Step 516/722 - Losses: 0.1315208226442337 + 0.06692459434270859\n",
            "\t- Step 517/722 - Losses: 0.10000517964363098 + 0.042936451733112335\n",
            "\t- Step 518/722 - Losses: 0.12987622618675232 + 0.058525145053863525\n",
            "\t- Step 519/722 - Losses: 0.0951937884092331 + 0.04341341555118561\n",
            "\t- Step 520/722 - Losses: 0.09265497326850891 + 0.03857554495334625\n",
            "\t- Step 521/722 - Losses: 0.11483104526996613 + 0.05579867959022522\n",
            "\t- Step 522/722 - Losses: 0.13685989379882812 + 0.07191582024097443\n",
            "\t- Step 523/722 - Losses: 0.17135033011436462 + 0.08939890563488007\n",
            "\t- Step 524/722 - Losses: 0.12230639904737473 + 0.055396683514118195\n",
            "\t- Step 525/722 - Losses: 0.07652847468852997 + 0.02500763349235058\n",
            "\t- Step 526/722 - Losses: 0.15020112693309784 + 0.07465463876724243\n",
            "\t- Step 527/722 - Losses: 0.10823691636323929 + 0.05210387334227562\n",
            "\t- Step 528/722 - Losses: 0.08612916618585587 + 0.036771632730960846\n",
            "\t- Step 529/722 - Losses: 0.05706365033984184 + 0.01392517238855362\n",
            "\t- Step 530/722 - Losses: 0.10246683657169342 + 0.03426434099674225\n",
            "\t- Step 531/722 - Losses: 0.0646323710680008 + 0.026438476517796516\n",
            "\t- Step 532/722 - Losses: 0.1165420413017273 + 0.04409817233681679\n",
            "\t- Step 533/722 - Losses: 0.09998250007629395 + 0.04182928800582886\n",
            "\t- Step 534/722 - Losses: 0.1663210242986679 + 0.07969056814908981\n",
            "\t- Step 535/722 - Losses: 0.09385798871517181 + 0.040175363421440125\n",
            "\t- Step 536/722 - Losses: 0.15577885508537292 + 0.076640784740448\n",
            "\t- Step 537/722 - Losses: 0.09770803153514862 + 0.04446126148104668\n",
            "\t- Step 538/722 - Losses: 0.06993832439184189 + 0.02526937797665596\n",
            "\t- Step 539/722 - Losses: 0.10889680683612823 + 0.04396499693393707\n",
            "\t- Step 540/722 - Losses: 0.07680701464414597 + 0.03550063073635101\n",
            "\t- Step 541/722 - Losses: 0.0917726680636406 + 0.04329022020101547\n",
            "\t- Step 542/722 - Losses: 0.11926078051328659 + 0.06203533709049225\n",
            "\t- Step 543/722 - Losses: 0.10157697647809982 + 0.04837547242641449\n",
            "\t- Step 544/722 - Losses: 0.07692837715148926 + 0.035268619656562805\n",
            "\t- Step 545/722 - Losses: 0.07248055189847946 + 0.017082370817661285\n",
            "\t- Step 546/722 - Losses: 0.09660525619983673 + 0.04806354641914368\n",
            "\t- Step 547/722 - Losses: 0.12253683805465698 + 0.04964808002114296\n",
            "\t- Step 548/722 - Losses: 0.11089456081390381 + 0.05756024271249771\n",
            "\t- Step 549/722 - Losses: 0.07138001918792725 + 0.01996895670890808\n",
            "\t- Step 550/722 - Losses: 0.09037637710571289 + 0.03972010314464569\n",
            "\t- Step 551/722 - Losses: 0.14076043665409088 + 0.06199733167886734\n",
            "\t- Step 552/722 - Losses: 0.10493571311235428 + 0.04556537792086601\n",
            "\t- Step 553/722 - Losses: 0.09859409928321838 + 0.04079513996839523\n",
            "\t- Step 554/722 - Losses: 0.11341774463653564 + 0.05631183087825775\n",
            "\t- Step 555/722 - Losses: 0.1012510135769844 + 0.03943706303834915\n",
            "\t- Step 556/722 - Losses: 0.1371254026889801 + 0.05878843367099762\n",
            "\t- Step 557/722 - Losses: 0.11307601630687714 + 0.05338680371642113\n",
            "\t- Step 558/722 - Losses: 0.11282940208911896 + 0.046942658722400665\n",
            "\t- Step 559/722 - Losses: 0.20475167036056519 + 0.10201306641101837\n",
            "\t- Step 560/722 - Losses: 0.13960152864456177 + 0.08051043748855591\n",
            "\t- Step 561/722 - Losses: 0.10829658061265945 + 0.058001577854156494\n",
            "\t- Step 562/722 - Losses: 0.05405723303556442 + 0.017048871144652367\n",
            "\t- Step 563/722 - Losses: 0.08265548944473267 + 0.036143552511930466\n",
            "\t- Step 564/722 - Losses: 0.11775408685207367 + 0.062174372375011444\n",
            "\t- Step 565/722 - Losses: 0.11067375540733337 + 0.04765244573354721\n",
            "\t- Step 566/722 - Losses: 0.10695518553256989 + 0.049533650279045105\n",
            "\t- Step 567/722 - Losses: 0.06363258510828018 + 0.022240176796913147\n",
            "\t- Step 568/722 - Losses: 0.08874672651290894 + 0.030663438141345978\n",
            "\t- Step 569/722 - Losses: 0.11420142650604248 + 0.053611982613801956\n",
            "\t- Step 570/722 - Losses: 0.10329422354698181 + 0.035167351365089417\n",
            "\t- Step 571/722 - Losses: 0.08187198638916016 + 0.03482595086097717\n",
            "\t- Step 572/722 - Losses: 0.1206662729382515 + 0.05977911129593849\n",
            "\t- Step 573/722 - Losses: 0.10771890729665756 + 0.04239680618047714\n",
            "\t- Step 574/722 - Losses: 0.10029064863920212 + 0.03786604106426239\n",
            "\t- Step 575/722 - Losses: 0.13826429843902588 + 0.05755818262696266\n",
            "\t- Step 576/722 - Losses: 0.08794102072715759 + 0.03486034274101257\n",
            "\t- Step 577/722 - Losses: 0.059901103377342224 + 0.02032371237874031\n",
            "\t- Step 578/722 - Losses: 0.12265578657388687 + 0.06178639084100723\n",
            "\t- Step 579/722 - Losses: 0.06913590431213379 + 0.027492588385939598\n",
            "\t- Step 580/722 - Losses: 0.07089520990848541 + 0.03693230822682381\n",
            "\t- Step 581/722 - Losses: 0.11946166306734085 + 0.05340724438428879\n",
            "\t- Step 582/722 - Losses: 0.12294362485408783 + 0.0648370087146759\n",
            "\t- Step 583/722 - Losses: 0.08547164499759674 + 0.025850733742117882\n",
            "\t- Step 584/722 - Losses: 0.0941276103258133 + 0.04342890530824661\n",
            "\t- Step 585/722 - Losses: 0.06766016781330109 + 0.03365388885140419\n",
            "\t- Step 586/722 - Losses: 0.16200850903987885 + 0.08708460628986359\n",
            "\t- Step 587/722 - Losses: 0.15092751383781433 + 0.06814213842153549\n",
            "\t- Step 588/722 - Losses: 0.11912915110588074 + 0.06897030025720596\n",
            "\t- Step 589/722 - Losses: 0.07696686685085297 + 0.02940920926630497\n",
            "\t- Step 590/722 - Losses: 0.096697136759758 + 0.04704343527555466\n",
            "\t- Step 591/722 - Losses: 0.155554860830307 + 0.06623078137636185\n",
            "\t- Step 592/722 - Losses: 0.11384016275405884 + 0.05743429437279701\n",
            "\t- Step 593/722 - Losses: 0.1475040167570114 + 0.06371809542179108\n",
            "\t- Step 594/722 - Losses: 0.05751698464155197 + 0.020391888916492462\n",
            "\t- Step 595/722 - Losses: 0.0779360756278038 + 0.026430074125528336\n",
            "\t- Step 596/722 - Losses: 0.07887522876262665 + 0.033086150884628296\n",
            "\t- Step 597/722 - Losses: 0.09455285966396332 + 0.036506108939647675\n",
            "\t- Step 598/722 - Losses: 0.0977630764245987 + 0.04185625910758972\n",
            "\t- Step 599/722 - Losses: 0.07263891398906708 + 0.03167293220758438\n",
            "\t- Step 600/722 - Losses: 0.12706057727336884 + 0.06032268702983856\n",
            "\t- Step 601/722 - Losses: 0.1085057333111763 + 0.05963537096977234\n",
            "\t- Step 602/722 - Losses: 0.05325303599238396 + 0.016453290358185768\n",
            "\t- Step 603/722 - Losses: 0.1479911357164383 + 0.06623606383800507\n",
            "\t- Step 604/722 - Losses: 0.07678880542516708 + 0.03170864284038544\n",
            "\t- Step 605/722 - Losses: 0.11111139506101608 + 0.05675338953733444\n",
            "\t- Step 606/722 - Losses: 0.14905574917793274 + 0.06836053729057312\n",
            "\t- Step 607/722 - Losses: 0.05927574634552002 + 0.018546419218182564\n",
            "\t- Step 608/722 - Losses: 0.12646229565143585 + 0.07898347079753876\n",
            "\t- Step 609/722 - Losses: 0.09534082561731339 + 0.04764152318239212\n",
            "\t- Step 610/722 - Losses: 0.04694966599345207 + 0.01115117222070694\n",
            "\t- Step 611/722 - Losses: 0.09121465682983398 + 0.04648742079734802\n",
            "\t- Step 612/722 - Losses: 0.05551999434828758 + 0.01495843380689621\n",
            "\t- Step 613/722 - Losses: 0.1365537941455841 + 0.06531654298305511\n",
            "\t- Step 614/722 - Losses: 0.13727664947509766 + 0.0673714131116867\n",
            "\t- Step 615/722 - Losses: 0.14002899825572968 + 0.06740731000900269\n",
            "\t- Step 616/722 - Losses: 0.07791291177272797 + 0.03847187012434006\n",
            "\t- Step 617/722 - Losses: 0.12046799063682556 + 0.05370829999446869\n",
            "\t- Step 618/722 - Losses: 0.17950448393821716 + 0.10152466595172882\n",
            "\t- Step 619/722 - Losses: 0.06559789180755615 + 0.023722685873508453\n",
            "\t- Step 620/722 - Losses: 0.19780001044273376 + 0.09393800795078278\n",
            "\t- Step 621/722 - Losses: 0.11154336482286453 + 0.05736183375120163\n",
            "\t- Step 622/722 - Losses: 0.1315729022026062 + 0.06733547896146774\n",
            "\t- Step 623/722 - Losses: 0.05059538409113884 + 0.014811597764492035\n",
            "\t- Step 624/722 - Losses: 0.11647351831197739 + 0.0531759113073349\n",
            "\t- Step 625/722 - Losses: 0.12843646109104156 + 0.05337074398994446\n",
            "\t- Step 626/722 - Losses: 0.06907491385936737 + 0.03309955820441246\n",
            "\t- Step 627/722 - Losses: 0.08025356382131577 + 0.03483166545629501\n",
            "\t- Step 628/722 - Losses: 0.1021055355668068 + 0.04071446508169174\n",
            "\t- Step 629/722 - Losses: 0.13236506283283234 + 0.0737810879945755\n",
            "\t- Step 630/722 - Losses: 0.092178575694561 + 0.04009734094142914\n",
            "\t- Step 631/722 - Losses: 0.18238162994384766 + 0.08123046159744263\n",
            "\t- Step 632/722 - Losses: 0.08212549984455109 + 0.03642174229025841\n",
            "\t- Step 633/722 - Losses: 0.09032560884952545 + 0.038238897919654846\n",
            "\t- Step 634/722 - Losses: 0.13638463616371155 + 0.06497315317392349\n",
            "\t- Step 635/722 - Losses: 0.05545306205749512 + 0.024605434387922287\n",
            "\t- Step 636/722 - Losses: 0.10725700110197067 + 0.046676598489284515\n",
            "\t- Step 637/722 - Losses: 0.1622743457555771 + 0.0811903327703476\n",
            "\t- Step 638/722 - Losses: 0.13554343581199646 + 0.07132026553153992\n",
            "\t- Step 639/722 - Losses: 0.08920158445835114 + 0.04089861363172531\n",
            "\t- Step 640/722 - Losses: 0.17348751425743103 + 0.10594400763511658\n",
            "\t- Step 641/722 - Losses: 0.17458543181419373 + 0.0931609645485878\n",
            "\t- Step 642/722 - Losses: 0.11696602404117584 + 0.05175480991601944\n",
            "\t- Step 643/722 - Losses: 0.1409931182861328 + 0.07997021824121475\n",
            "\t- Step 644/722 - Losses: 0.11894874274730682 + 0.05388323590159416\n",
            "\t- Step 645/722 - Losses: 0.09689511358737946 + 0.04383097216486931\n",
            "\t- Step 646/722 - Losses: 0.06499125063419342 + 0.023602675646543503\n",
            "\t- Step 647/722 - Losses: 0.05334877967834473 + 0.01487591490149498\n",
            "\t- Step 648/722 - Losses: 0.16680872440338135 + 0.07042773813009262\n",
            "\t- Step 649/722 - Losses: 0.1673261821269989 + 0.09520449489355087\n",
            "\t- Step 650/722 - Losses: 0.09823114424943924 + 0.04992618039250374\n",
            "\t- Step 651/722 - Losses: 0.1105712279677391 + 0.051247913390398026\n",
            "\t- Step 652/722 - Losses: 0.10801520198583603 + 0.053374841809272766\n",
            "\t- Step 653/722 - Losses: 0.11963748186826706 + 0.04957885295152664\n",
            "\t- Step 654/722 - Losses: 0.12099398672580719 + 0.045457690954208374\n",
            "\t- Step 655/722 - Losses: 0.06837869435548782 + 0.02945854514837265\n",
            "\t- Step 656/722 - Losses: 0.13217340409755707 + 0.06502358615398407\n",
            "\t- Step 657/722 - Losses: 0.09215916693210602 + 0.04554840922355652\n",
            "\t- Step 658/722 - Losses: 0.08143077045679092 + 0.036508820950984955\n",
            "\t- Step 659/722 - Losses: 0.1988307386636734 + 0.10172373056411743\n",
            "\t- Step 660/722 - Losses: 0.13579142093658447 + 0.06261150538921356\n",
            "\t- Step 661/722 - Losses: 0.07185912877321243 + 0.026455221697688103\n",
            "\t- Step 662/722 - Losses: 0.06778791546821594 + 0.031288743019104004\n",
            "\t- Step 663/722 - Losses: 0.12593455612659454 + 0.062050361186265945\n",
            "\t- Step 664/722 - Losses: 0.11591089516878128 + 0.05407968908548355\n",
            "\t- Step 665/722 - Losses: 0.07714127004146576 + 0.03476880490779877\n",
            "\t- Step 666/722 - Losses: 0.0917520523071289 + 0.038911812007427216\n",
            "\t- Step 667/722 - Losses: 0.06948146969079971 + 0.027754968032240868\n",
            "\t- Step 668/722 - Losses: 0.13007019460201263 + 0.060600969940423965\n",
            "\t- Step 669/722 - Losses: 0.1369630992412567 + 0.06206083297729492\n",
            "\t- Step 670/722 - Losses: 0.1271917223930359 + 0.060836561024188995\n",
            "\t- Step 671/722 - Losses: 0.07294133305549622 + 0.027039313688874245\n",
            "\t- Step 672/722 - Losses: 0.14154484868049622 + 0.07008896768093109\n",
            "\t- Step 673/722 - Losses: 0.054991431534290314 + 0.02166888862848282\n",
            "\t- Step 674/722 - Losses: 0.12489709258079529 + 0.05113964527845383\n",
            "\t- Step 675/722 - Losses: 0.09117013961076736 + 0.02927173115313053\n",
            "\t- Step 676/722 - Losses: 0.0636012852191925 + 0.02436022087931633\n",
            "\t- Step 677/722 - Losses: 0.08563098311424255 + 0.04340369626879692\n",
            "\t- Step 678/722 - Losses: 0.10808225721120834 + 0.05836478993296623\n",
            "\t- Step 679/722 - Losses: 0.08388146013021469 + 0.03818260505795479\n",
            "\t- Step 680/722 - Losses: 0.14687910676002502 + 0.0835808590054512\n",
            "\t- Step 681/722 - Losses: 0.11485747247934341 + 0.05942748486995697\n",
            "\t- Step 682/722 - Losses: 0.14682230353355408 + 0.08088397234678268\n",
            "\t- Step 683/722 - Losses: 0.08281683176755905 + 0.03199385106563568\n",
            "\t- Step 684/722 - Losses: 0.08976393938064575 + 0.042165450751781464\n",
            "\t- Step 685/722 - Losses: 0.08407090604305267 + 0.03270656615495682\n",
            "\t- Step 686/722 - Losses: 0.12804558873176575 + 0.07275226712226868\n",
            "\t- Step 687/722 - Losses: 0.0877426490187645 + 0.04449034482240677\n",
            "\t- Step 688/722 - Losses: 0.14419347047805786 + 0.07751163840293884\n",
            "\t- Step 689/722 - Losses: 0.10046841204166412 + 0.04468368738889694\n",
            "\t- Step 690/722 - Losses: 0.11476997286081314 + 0.05655819550156593\n",
            "\t- Step 691/722 - Losses: 0.11706569790840149 + 0.060790956020355225\n",
            "\t- Step 692/722 - Losses: 0.10769885033369064 + 0.05291464179754257\n",
            "\t- Step 693/722 - Losses: 0.09651103615760803 + 0.03281617909669876\n",
            "\t- Step 694/722 - Losses: 0.05768902972340584 + 0.024681389331817627\n",
            "\t- Step 695/722 - Losses: 0.10784415900707245 + 0.05439669266343117\n",
            "\t- Step 696/722 - Losses: 0.10824242234230042 + 0.04321007430553436\n",
            "\t- Step 697/722 - Losses: 0.07051625847816467 + 0.03041079267859459\n",
            "\t- Step 698/722 - Losses: 0.04640103876590729 + 0.013404836878180504\n",
            "\t- Step 699/722 - Losses: 0.13198088109493256 + 0.06521984934806824\n",
            "\t- Step 700/722 - Losses: 0.13745741546154022 + 0.07704004645347595\n",
            "\t- Step 701/722 - Losses: 0.07588709890842438 + 0.02950468100607395\n",
            "\t- Step 702/722 - Losses: 0.07664044946432114 + 0.034102216362953186\n",
            "\t- Step 703/722 - Losses: 0.09503591060638428 + 0.04334848374128342\n",
            "\t- Step 704/722 - Losses: 0.09343196451663971 + 0.038263194262981415\n",
            "\t- Step 705/722 - Losses: 0.057257525622844696 + 0.024615861475467682\n",
            "\t- Step 706/722 - Losses: 0.11366713047027588 + 0.052603933960199356\n",
            "\t- Step 707/722 - Losses: 0.06645633280277252 + 0.02701535075902939\n",
            "\t- Step 708/722 - Losses: 0.12493262439966202 + 0.06382853537797928\n",
            "\t- Step 709/722 - Losses: 0.0729430764913559 + 0.02043382078409195\n",
            "\t- Step 710/722 - Losses: 0.1874949336051941 + 0.108201764523983\n",
            "\t- Step 711/722 - Losses: 0.10136939585208893 + 0.055959440767765045\n",
            "\t- Step 712/722 - Losses: 0.10343889892101288 + 0.0418892502784729\n",
            "\t- Step 713/722 - Losses: 0.08893982321023941 + 0.03937976807355881\n",
            "\t- Step 714/722 - Losses: 0.08751572668552399 + 0.038343869149684906\n",
            "\t- Step 715/722 - Losses: 0.11295630037784576 + 0.060770634561777115\n",
            "\t- Step 716/722 - Losses: 0.09253500401973724 + 0.04208369553089142\n",
            "\t- Step 717/722 - Losses: 0.21169184148311615 + 0.09982217848300934\n",
            "\t- Step 718/722 - Losses: 0.09453295171260834 + 0.047842659056186676\n",
            "\t- Step 719/722 - Losses: 0.07358086109161377 + 0.03351232409477234\n",
            "\t- Step 720/722 - Losses: 0.15725067257881165 + 0.07372993975877762\n",
            "\t- Step 721/722 - Losses: 0.12024545669555664 + 0.05856828764081001\n",
            "\t- Step 722/722 - Losses: 0.10757636278867722 + 0.06951301544904709\n",
            "Classifier Loss: 128.4532928466797\n",
            "BBox Regression Loss: 36.25412368774414\n",
            "Took 1660.1727442741394s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "torch.save(model.state_dict(), \"./finetuned-faster-rcnn.pth\")"
      ],
      "metadata": {
        "id": "cCgJRfS_OfMV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reload\n",
        "model.load_state_dict(torch.load(\"./finetuned-faster-rcnn.pth\", weights_only=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-u9bpHiOpj1",
        "outputId": "ad1bfd17-919d-4442-a8ed-c958413485ba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UjhbMjAPjHT",
        "outputId": "80e1fcbb-765e-4591-a598-9229dda79107"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0-3): 4 x Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): FastRCNNConvFCHead(\n",
              "      (0): Conv2dNormActivation(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): Conv2dNormActivation(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Conv2dNormActivation(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Conv2dNormActivation(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Flatten(start_dim=1, end_dim=-1)\n",
              "      (5): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (6): ReLU(inplace=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=21, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=84, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_loss = 0\n",
        "bbox_loss = 0\n",
        "X = []\n",
        "y = []\n",
        "y_preds = []\n",
        "for data in dataloader_dataset:\n",
        "        for d in data:\n",
        "            objs = d[1]['annotation']['object']\n",
        "            bboxes = []\n",
        "            labels = []\n",
        "            for i in range(len(objs)):\n",
        "                bbox_dict = objs[i]['bndbox']\n",
        "                bbox = [int(bbox_dict[key]) for key in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
        "                bboxes.append(bbox)\n",
        "\n",
        "                label = objs[i]['name']\n",
        "                label = class_to_index[label]\n",
        "                labels.append(label)\n",
        "\n",
        "            bboxes = torch.as_tensor(bboxes, dtype=torch.int64)\n",
        "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "            X.append(d[0].cuda())\n",
        "            y.append({\n",
        "                'boxes': bboxes.cuda(),\n",
        "                'labels': labels.cuda(),\n",
        "            })\n",
        "\n",
        "        y_preds = model(X)\n",
        "        break # only loop for 1 batch"
      ],
      "metadata": {
        "id": "n8wAjz337lt1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT_hrQ3FRDS1",
        "outputId": "7ad257a0-69f0-4daa-c0e2-57533e8a31e4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': tensor([[  1, 215, 103, 287],\n",
              "         [ 23, 195,  55, 218]], device='cuda:0'),\n",
              " 'labels': tensor([8, 8], device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPL1YnLWR-fw",
        "outputId": "fef6913f-1c2b-4a87-b409-cf28584f35f3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': tensor([[185.0494, 114.7610, 194.3344, 124.6680],\n",
              "         [182.6682, 116.0456, 192.1408, 123.5601],\n",
              "         [129.4860, 143.7688, 193.9598, 186.5196],\n",
              "         [  3.4847, 109.3897,  24.7011, 130.3596],\n",
              "         [284.0595,   8.6814, 298.8501, 152.1658],\n",
              "         [180.2316, 113.9339, 194.6161, 131.0658],\n",
              "         [115.2738, 131.2514, 201.0613, 199.6128],\n",
              "         [204.9786, 157.8374, 213.5469, 179.5437],\n",
              "         [145.7702, 173.2375, 193.5998, 211.5951],\n",
              "         [ 30.3494, 139.3583, 160.5053, 206.2996],\n",
              "         [191.2969,  27.0418, 199.0635,  34.3089],\n",
              "         [275.8835, 131.6127, 298.4948, 188.8290],\n",
              "         [276.4227, 155.6070, 293.3477, 189.3974],\n",
              "         [190.1504,  23.3716, 200.9082,  36.1661],\n",
              "         [  8.1414, 109.4583,  23.2716, 124.0882],\n",
              "         [  0.0000, 112.6425,  38.9359, 151.6787],\n",
              "         [265.9276, 139.8323, 284.7711, 148.8715],\n",
              "         [114.3245, 159.9930, 178.9964, 187.3891],\n",
              "         [182.1668, 115.7636, 193.0047, 123.8126],\n",
              "         [179.6382, 116.5003, 187.4305, 132.6884],\n",
              "         [230.9124,  62.6492, 244.3759,  75.9587],\n",
              "         [272.2018, 142.4676, 294.9182, 162.4260],\n",
              "         [129.9538, 167.6686, 172.5465, 213.5392],\n",
              "         [ 36.4050,   2.8114, 275.9125, 195.5072],\n",
              "         [ 10.2526, 174.8878,  76.1649, 208.7283],\n",
              "         [106.5737, 145.8117, 126.1241, 200.3672],\n",
              "         [191.6260,  18.9199, 196.8313,  31.1249],\n",
              "         [248.0490, 109.8215, 298.6165, 143.1426],\n",
              "         [ 25.6155, 151.8883,  38.9613, 165.7110],\n",
              "         [179.5043, 115.7872, 186.5019, 132.4736],\n",
              "         [  4.9352, 177.3717,  46.9741, 194.6423],\n",
              "         [ 60.2223, 154.8332,  96.1700, 214.3446],\n",
              "         [180.0535, 121.3739, 191.4796, 143.9931],\n",
              "         [ 81.7886, 168.9781, 103.2607, 191.0668],\n",
              "         [ 44.2020,  82.3536, 119.9897, 228.0471],\n",
              "         [207.0115, 163.2849, 212.8875, 181.0350],\n",
              "         [ 19.4877, 149.6328,  39.5500, 168.6665],\n",
              "         [  5.5581, 191.7976,  41.8463, 207.6561],\n",
              "         [ 31.7556, 143.0162,  41.5508, 159.3563],\n",
              "         [ 28.7785, 113.2864, 251.3380, 232.9712],\n",
              "         [124.5349,  92.3278, 148.0619, 113.4313],\n",
              "         [106.9051, 146.3851, 131.3403, 209.5471],\n",
              "         [ 39.1993, 158.4816,  53.8578, 169.6876],\n",
              "         [ 52.2601, 145.3955, 102.1967, 176.6697],\n",
              "         [200.1945, 178.6510, 210.1378, 193.7029],\n",
              "         [197.8570, 157.5338, 211.9429, 187.9373],\n",
              "         [222.5378,  94.0620, 240.2089, 121.5388],\n",
              "         [123.6796,  56.6828, 173.0818, 107.7382],\n",
              "         [  2.8801, 169.1684,  73.9678, 245.4629],\n",
              "         [274.0828, 160.6404, 281.3481, 176.4786],\n",
              "         [144.1793,  71.5803, 191.2606, 123.2137],\n",
              "         [ 58.7248, 146.5420,  75.5206, 161.7836],\n",
              "         [ 63.4771, 146.2301, 113.3835, 179.2877],\n",
              "         [ 60.5532, 165.0146, 112.2841, 202.2731],\n",
              "         [217.3458,  57.0879, 256.4838,  90.5659],\n",
              "         [112.7236,  82.1444, 156.6817, 124.8241],\n",
              "         [  1.9730, 178.3229,  42.4868, 209.8556],\n",
              "         [  7.5093, 204.6707,  25.6530, 217.6056],\n",
              "         [230.9639,  63.2691, 253.1803,  95.0066],\n",
              "         [  0.0000, 136.7518, 184.6748, 290.9287],\n",
              "         [ 38.5104, 160.5387,  55.6262, 174.0198],\n",
              "         [ 14.2124, 171.5889,  61.3091, 206.1136],\n",
              "         [266.3611, 159.4236, 283.2271, 188.3446],\n",
              "         [248.3233, 159.7301, 257.5428, 176.9972],\n",
              "         [167.3026, 207.4045, 180.2007, 223.1608],\n",
              "         [ 12.6038, 158.9940,  58.6580, 199.2635],\n",
              "         [245.3478, 143.5366, 286.6625, 187.1563],\n",
              "         [181.5118, 116.2625, 190.9261, 124.6599],\n",
              "         [148.9180, 204.8327, 167.3529, 225.5912],\n",
              "         [196.7451, 155.8474, 213.1192, 177.0964],\n",
              "         [104.6900, 147.4379, 183.0424, 175.6311],\n",
              "         [ 22.7233, 142.1530,  38.1002, 156.3882],\n",
              "         [247.3752, 153.5099, 291.2535, 189.9845],\n",
              "         [223.5815, 108.7202, 239.7909, 136.6701],\n",
              "         [ 12.1763, 184.0158,  69.6642, 220.7208],\n",
              "         [159.8813, 197.0750, 181.3712, 214.8897],\n",
              "         [204.2182, 155.7072, 211.2031, 162.3532],\n",
              "         [224.0092, 126.4126, 239.0250, 154.2692],\n",
              "         [276.4161, 165.8128, 283.2347, 183.4859],\n",
              "         [258.2458, 109.6037, 300.0000, 175.0140],\n",
              "         [ 11.9105, 147.0082,  34.6089, 166.4793],\n",
              "         [ 59.8237, 168.1310,  83.3156, 191.4710],\n",
              "         [119.5445,  93.7991, 138.8191, 122.1713],\n",
              "         [263.6742, 145.7187, 278.8197, 180.5702],\n",
              "         [ 14.8614, 153.3297,  29.2635, 168.1458],\n",
              "         [219.0461, 136.3972, 236.7622, 166.3726],\n",
              "         [168.6047, 198.8414, 213.1743, 233.8323],\n",
              "         [221.1991,  66.5879, 256.5501, 111.4989],\n",
              "         [  4.2876, 162.6637,  69.3835, 223.6064],\n",
              "         [150.7428, 213.9654, 164.0916, 229.0612],\n",
              "         [202.6922, 156.3954, 209.3551, 163.9775],\n",
              "         [183.7713, 195.2013, 224.8983, 228.2557],\n",
              "         [  9.7170, 204.0490,  25.5900, 217.1648],\n",
              "         [203.0036, 157.0814, 209.5272, 163.7032],\n",
              "         [128.2973, 193.3294, 142.4014, 222.0291],\n",
              "         [193.3661,  18.4987, 199.7587,  34.3559],\n",
              "         [178.2078, 190.0660, 193.5031, 206.7111],\n",
              "         [125.6615, 202.6500, 300.0000, 247.1411],\n",
              "         [215.8959, 125.7772, 240.3410, 175.4957],\n",
              "         [ 57.8755, 221.4304,  79.1688, 239.4003]], device='cuda:0',\n",
              "        grad_fn=<StackBackward0>),\n",
              " 'labels': tensor([ 8,  8,  3, 13, 13,  8,  3, 13, 13, 10,  8, 13, 13,  7, 13, 13,  3,  3,\n",
              "          3,  2,  3, 13, 18, 13, 10, 18,  8, 10, 13,  8, 18, 18,  1, 13,  3, 13,\n",
              "         13,  3, 13, 18, 13, 10,  3, 18, 13, 13, 18, 18,  8,  3, 10, 18,  3,  9,\n",
              "          3, 13, 19, 13, 13, 13, 13, 19, 13, 13,  9,  3, 18,  1,  9, 10,  3, 13,\n",
              "         13, 18,  8, 13, 14, 18, 13, 13, 13, 18, 13, 18, 13, 13, 13, 13,  3, 13,\n",
              "         14, 13,  3,  2, 18,  8, 13, 18, 13, 13], device='cuda:0'),\n",
              " 'scores': tensor([0.1571, 0.1307, 0.1174, 0.1125, 0.1123, 0.1121, 0.1086, 0.1083, 0.1080,\n",
              "         0.1078, 0.1069, 0.1058, 0.1057, 0.1054, 0.1050, 0.1040, 0.1038, 0.1034,\n",
              "         0.1027, 0.1006, 0.0986, 0.0966, 0.0964, 0.0960, 0.0959, 0.0956, 0.0956,\n",
              "         0.0951, 0.0947, 0.0946, 0.0941, 0.0940, 0.0938, 0.0936, 0.0936, 0.0936,\n",
              "         0.0935, 0.0934, 0.0932, 0.0931, 0.0927, 0.0926, 0.0925, 0.0924, 0.0923,\n",
              "         0.0918, 0.0918, 0.0918, 0.0915, 0.0914, 0.0913, 0.0912, 0.0912, 0.0906,\n",
              "         0.0904, 0.0902, 0.0900, 0.0899, 0.0899, 0.0899, 0.0898, 0.0898, 0.0897,\n",
              "         0.0897, 0.0897, 0.0896, 0.0895, 0.0893, 0.0891, 0.0890, 0.0889, 0.0888,\n",
              "         0.0887, 0.0887, 0.0887, 0.0887, 0.0886, 0.0885, 0.0884, 0.0882, 0.0882,\n",
              "         0.0882, 0.0881, 0.0879, 0.0878, 0.0878, 0.0877, 0.0874, 0.0873, 0.0871,\n",
              "         0.0871, 0.0871, 0.0869, 0.0869, 0.0869, 0.0869, 0.0868, 0.0868, 0.0867,\n",
              "         0.0867], device='cuda:0', grad_fn=<IndexBackward0>)}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fq3HMgTRa-q",
        "outputId": "282bac10-a80d-4d27-c770-2d61e84845dc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.5.1+cu124)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.12.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.12.0-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.12.0 torchmetrics-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.detection import MeanAveragePrecision\n",
        "\n",
        "mAP = MeanAveragePrecision()\n",
        "\n",
        "preds = [dict(\n",
        "    boxes=y_preds[0]['boxes'],\n",
        "    labels=y_preds[0]['labels'],\n",
        "    scores=y_preds[0]['scores'],\n",
        ")]\n",
        "targets = [dict(\n",
        "    boxes=y[0]['boxes'],\n",
        "    labels=y[0]['labels'],\n",
        ")]\n",
        "\n",
        "mAP.update(preds, targets)\n",
        "\n",
        "mAP.compute()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-vehh4DL-9i",
        "outputId": "a530ebe3-618e-4203-e1b7-8096a9ba8a2f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'map': tensor(0.),\n",
              " 'map_50': tensor(0.),\n",
              " 'map_75': tensor(0.),\n",
              " 'map_small': tensor(0.),\n",
              " 'map_medium': tensor(0.),\n",
              " 'map_large': tensor(-1.),\n",
              " 'mar_1': tensor(0.),\n",
              " 'mar_10': tensor(0.),\n",
              " 'mar_100': tensor(0.),\n",
              " 'mar_small': tensor(0.),\n",
              " 'mar_medium': tensor(0.),\n",
              " 'mar_large': tensor(-1.),\n",
              " 'map_per_class': tensor(-1.),\n",
              " 'mar_100_per_class': tensor(-1.),\n",
              " 'classes': tensor([ 1,  2,  3,  7,  8,  9, 10, 13, 14, 18, 19], dtype=torch.int32)}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.detection import IntersectionOverUnion\n",
        "\n",
        "iou = IntersectionOverUnion()\n",
        "preds = [dict(\n",
        "    boxes=y_preds[0]['boxes'],\n",
        "    labels=y_preds[0]['labels']\n",
        ")]\n",
        "targets = [dict(\n",
        "    boxes=y[0]['boxes'],\n",
        "    labels=y[0]['labels']\n",
        ")]\n",
        "\n",
        "iou.update(preds, targets)\n",
        "iou.compute()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6x3T7W1R7yp",
        "outputId": "03978e97-8c75-4800-ab55-cddf2aa75187"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'iou': tensor(0.0403, device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pzEX-mBWT-yH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}